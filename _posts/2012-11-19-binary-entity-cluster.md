---
layout: default
published: true
---

# 二元组实体（数据）聚类  

以前没有接触过聚类的工作，这是我第一次做聚类，在显示环境下的数据聚类要得出理想的效果，其难度远远大于我原来的想象，在过去一周我无数次因为不知所措而感到很沮丧，直到现在才能做出相对有效的聚类，觉得有必要为这一周多的苦逼写下一篇记录。  

## 背景
跟标题一样，这个聚类过程的输入是一对一对的二元组数据，在我的工作里面是(userID, shopID)，表示用户喜欢过这个商店。而聚类的期望目标就是按照把商店按照用户的喜好特点来聚类，也就是每个聚类中的商店，都具有代表一类用户偏好的特征。刚开始给出的目标是对(用户，商店)这个二分图进行聚类，同时获得用户的聚类{Ui}和商店的聚类{Si}，使得Ui里的用户都喜欢Si里面的商店。在会议室讨论这个聚类的时候，Austin认为这种二分图聚类的问题在业界应该是被研究得很透彻，因此应该有现成的解决方案，而会后，他的确给我找到了这样的一篇论文，而我在这个事情上的工作就从这一片论文正式开始。之所以说正式开始，因为在会议之前其实我已经有了一定的设计，写了一个算法来做这个事情，当然用的不是论文里面的高深方法，而是下面就要说到的一个错误的方法。  

## 错误的算法
针对这个问题的输入，很容易可以构造出两个倒排索引：(shopID, userIDs)和(userID, shopIDs)。我基于一个思考：如果各自喜欢两个商店的用户集合的交集大小达到一个阀值以上，那么这两个商店就应该具有一定的共同特征，这个特征的细化程度取决于阀值的大小，阀值越大，特征越是细化。扩大商店聚类的方式就是迭代每一个已有的聚类（初始的时候就是每个商店看作一个已有聚类），如果能通过喜欢用户群交集大小的阀值，就并入到已有聚类中，形成新的聚类这个算法有以下几个问题：  
第一，这样在第一轮里面的计算量会很大，因为是个n次方的扩展（N为扩展次数），从一个店的聚类扩展到两个店，就有m\*m的可能候选聚类（m是商店的数量）；从两个店的聚类扩展到三个店，理论上就可能有m\*m\*m的处理复杂度。在一般情况下，如果阀值提高一点的话，经过第一轮扩展后，后续的扩展都不会达到很高的复杂度（通常都比第一轮扩展复杂度要低），因此性能问题不是这个算法最糟糕的地方。

第二，这个算法基于用户的交集来构造商店的聚类，而为了缓解上面第一个问题，使得程序能够在可接受的复杂度内完成计算，就必须让用户交集数量的阀值设置得比较高，在我的样本数据里面有105万用户和7800商店，在这个数据规模下经过测试我得起码把阀值设置到5000或以上才能让计算时长可接受。这样高的用户阀值会导致很严重的问题——使得由小众用户（5000以下的）构成的商店聚类在结果中消失，简单来说，因为为了让算法可执行，而丢弃了应该被注意到的聚类结果。并且在这个情况下最终计算出来的商店聚类也是基于大概5000用户的交集上，因为随着每个聚类扩展，共同的用户数量会锐减。而在实际结果下，聚类大小达到4个店铺就已经无法扩展了（也就是不存在5个店铺拥有5000以上共同的用户）。  

起初我以为在第二、三轮扩展的时候把阀值降低或许能缓解问题，没错，这样虽然可以让聚类大小增加一点（成了大小为6个聚类），但这样跟预期想要的还是差太远了，毕竟我们的聚类目标是希望，通过分析聚类中的商店共同的特征，假设成用户的爱好，然后对用户群进行划分，这样只有5，6个商店的聚类根本不能作为研究对象。这个算法通过对已有的聚类进行扩展，会得出这样的结果，{S1,S2,S3,S4}、{S1,S2,S3,S5}、{S2,S3,S4,S5}、{S1,S2,S4,S5}等等只相差一个商店的聚类结果，如果聚类大小更大的话，会产生更多这样的结果，但正如前面提到，才5、6个商店大小的聚类就无法再扩展了；基于这个情况，我选择把聚类给合并一下，顺便也通过这样扩大聚类的size，然后当我一合并就发现所有商店几乎都会merge到一起，原因是因为这些聚类之间本来相差的商店就不多，而构造过程都基于交集用户，于是最终导致一个结果——所有不同聚类中的商店其实都拥有一部分共同用户，在我的运行结果中大概是3000个。当我搞懂这一点以后，我彻底明白这个算法是错误的。  

## Leran From Papers
前面提到Austin帮我找了一份paper，叫做《Bipartite Graph Partitioning and Data Clustering》，里面给出一种算法可以同时对二分图中两边的元素进行*无重叠*聚类，并且给出了数学证明。以前我虽然看过paper，但直到这一次我真的想把paper中的知识直接应用到工程中我才发现自己的知识是多么脆弱。paper里面给出了算法的数学证明，一大堆的矩阵运算让我完全摸不着头脑，尽管我看懂了作者是根据什么来衡量聚类的相关度，但对于算法为什么正确，甚至连算法具体要计算什么，是怎么个流程我也无法清楚知道；如果不是要具体应用起来的话，我会以为自己已经大致能掌握这paper的内容，就差证明而已，但这次我就完全知道自己的天真了。  

在这份paper里面有一个核心的思想就是把这个问题的解决方式引导到解SVD的问题上，这就给我引了一个很大的谜团——什么是SVD？解决什么问题？在网上我随便都能找到很多SVD的文章啊，博客啊之类的，但我必须得承认没有一篇是能让我这种完全把线性代数还给老师的人能看懂的。最后我找到了一份23页的PDF，应该也算是一份paper，我在看过这份paper的introduction之后就认定它是我所寻找的！  

Most tutorials on complex topics are apparently written by very smart people whose goal is to use as little space as possible and who assume that their readers already know almost as much as the author does. This tutorial’s not like that. It’s more a manifestivus for the rest of us. It’s about the mechanics of singular value decomposition, especially as it relates to some techniques in natural language processing. It’s written by someone who knew zilch about singular value decomposition or any of the underlying math before he started writing it, and knows barely more than that now. Accordingly, it’s a bit long on the background part, and a bit short on the truly explanatory part, but hopefully it contains all the information necessary for someone who’s never heard of singular value decomposition before to be able to do it.

这份paper正文一共21页，但就花了13页来扫盲，把SVD涉及到的所有线性代数基础知识都集中用简单例子阐述了一遍，因此让我觉得之前看过的SVD资料都多余了，[如果一开始能找到这份paper](http://vdisk.weibo.com/s/hQiRd/1353311091)，会跳过不少痛苦的弯路。因为这paper作为教材已经非常好了，所以我也不觉得有必要在这份心得里面作什么补充，倒是跟自己总结一下还好。SVD（Singular Value Decomposition）可以把一个表示A-B两个概念的关联性的矩阵（从我的应用角度来说）分解成另一个矩阵表达式（这个表达式有三个矩阵），通过SVD可以加强原来已有的较强的关联性，同时削减原来已有较弱的关联性，具体做法就是在分解后的三个矩阵中，根据特征值矩阵识别哪些列（概念、维度）的特征值足够小，然后裁掉，裁掉这些列后的三个矩阵会变成很小的矩阵，然后重新构造成原来的矩阵（通过矩阵乘法）。我能掌握到的程度目前就只有这些，因为后面肯定还有很重要的一步，就是如何应用SVD重新构造的矩阵（裁剪维度后的分解矩阵）来实现原来要做的事情，这一点是我目前还没掌握的。  

单单掌握SVD的基本原理还不足够，我还需要知道在工程计算程序中一般是如何实现这个算法。于是我又找到了下面这些资料:：[《Matrix Computations》1](http://vdisk.weibo.com/s/hRTZX/1353312532)、[《Matrix Computations》2](http://vdisk.weibo.com/s/ioOGX/1353312888)、[《Optimal sorting algorithms for parallel computers》](http://vdisk.weibo.com/s/hVAHJ/1353312942)来学习串行的SVD，然后是并行的SVD，并行是必须的，因为在我的数据规模下不可能依靠单个节点进行计算，慢死了一头猪！到现在基本上这些原理我都理解了，但经过那份paper的教训，我认为只有当我实现过一个可用的并行SVD算法之后，才能认为自己掌握这些内容，因此为了以后可以更高效地去做这个聚类，下一步就是要做好这个实现。  

基本上可以总结一句，就是Austin给我的paper里面涉及的算法我没有实现，因为自己学不会，但我从里面互相关联的资料学到的知识，导致我马上要提到自己设计的一个可用的聚类算法。
