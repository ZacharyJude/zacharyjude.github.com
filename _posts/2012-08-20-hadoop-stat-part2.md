---
layout: default
published: true
---

# Hadoop统计平台——MMStat（二）  

## 数据结构

如果直接就把方案写出来，其实理解起来会有种“中国式教育”的味道。所以我还是打算先提出当时遇到的问题。  
如果现在我想统计2012-08-20这一天“美丽说”这个app的iphone平台上2.5.8版本的“最新宝贝”浏览页的PV数和设备数，那么要怎么解决这个问题？很明显我只需要输出当天日志文件的所有内容，然后分别grep：美丽说、iPhone、2.5.8、最新宝贝的浏览请求，最后提取一个日志数还有抽取其中的设备号并去重，那我就可以达到目标。  
  
那如果现在需要的是另一个版本2.5.9的同样的数据呢？好吧，我还是可以输出当天日志再重新grep一把，但问题很快就会升级到需要看各个版本的同一个数据，如果历史有过10个版本，那么同样的日志内容就会被输出10遍，然而真正的问题并不在这里，而是有可能不需要看分版本的数据，需要聚合起来，然而这一次变成要看不同的设备平台的数据，例如iphone、android和ipad；又或者除了不同的设备和不同的版本，PM、RD和leader们还需要看不同的app还有不单只是“最新宝贝”浏览请求的设备和PV，还有别的其他N个请求，说到这里基本上以我的语文水平，可能你已经有点晕，但无论晕还是不晕，该看的，该计算的还是得做出来。  
  
上述的问题其实就是一个维度组合的问题，首先简单说一个概念（我从没学过数据挖掘，如果你学过，相信你马上能理解我说的小玩意啦）：**维度和维度值**。根据上面实际要解决的问题背景，像app、设备平台、版本号、请求类型这些都被我称为维度，而这些维度的具体取值就是维度值。那么上述的问题其实就是根据不同的维度值组合起来进行计算。而MMStat的诞生第一个就是要解决这样一个问题——高效地计算不同维度值组合形成的条件下对应的不同统计项的统计结果（统计项指的就是PV、去重设备等等）。  
  
不知道你首先想到的是不是RDBMS查询，上面的条件不就是where从句里面的各个筛选条件嘛，既然用RDBMS就可以，为什么要专门设计一个系统呢？谈到这个问题就得追溯到我刚接手要负责无线统计时的光景。那时候美丽说的数据组就是用mysql存储解析后的日志，然后在统计计算的python脚本里面写了一大票sql，最后生成的统计结果再写入到mysql供给前段应用查询。其实这样的方案跟我最开始提到的不停输出全部日志内容，然后根据条件grep一把没什么区别，可能就是程序里面枚举各个条件要方便一点而已。然而这样的方案最大的问题就是日志的重复遍历，忘记在哪本经典的算法书（相信我，肯定不是国人写的）上看到过这么大致这么意思的一句话，“如果存在冗余计算，那么就有提升的算法效率的空间”，我一直非常信奉这句话，当然在实际工作上可能针对那一点冗余计算要付出很大的努力，例如设计复杂的存储结构和复杂的算法代码，因此还是得权衡。  
  
在当时的方案中，重复输出日志信息就是冗余计算，而且非常冗余，因为符合每一个维度值组合的条件筛选出来的日志记录其实只会是所有日志记录中的极小一部分，然而为了计算这么一部分的日志而遍历全部日志，是非常不明智的。因此当时刚起步的无线只有大概8G一天的日志，统计的内容也不足现在的1/10，但却每天要计算12个小时——这也是我要接手统计的原因，无论新方案要比旧方案复杂多少，也必须要改变。  
  
既然冗余的部分在于多次重复的日志全文遍历，那么当时我追求的就是一次过的遍历，然而一次过遍历就直接计算出要求的统计结果，这一点我当时怎么也没想出解决方案，在我的思考中，我追求当给出维度值组合后，可以高效地查找到需要的数据，然后进行聚合计算（原来的方案中，查找的方式就是全文遍历），而思考过后得到的方案就是用一个最详细的维度组合值来分类（索引）属于这个维度值组合下的数据。在上一篇文章里面我提到在MMStat以前，一个python写的单机统计系统一直沿用了半年，在那一个系统中我采用的方式就是顺序索引文件，为每一个维度值建立一个位置索引列表，直接索引到位于文本文件中日志数据，这样一旦给出多个维度的维度值，然后组合的方式就是索引值（数据在文本文件上的偏移量）集合求交集，的出来的结果就是符合维度值组合的数据索引集合。这种方式很高效，所以让那个系统可以在单机计算中沿用那么长的时间。为什么要提到python统计系统呢，因为他的设计对后面MMStat的设计有很多指导，而且好一些抽象的结构还是没有改变的。  
  
在这样的一个思路下，python统计系统的数据存储结构如下：
![python统计系统高层存储结构图](/assets/python_abstract_store_structrue.png)  
  
可惜顺序索引文件这一招在Hadoop的Map-Reduce处理框架下很难设计，起码我当时就没有想出来。但无论如何也不能因为Hadoop的分布式处理能力，我就回到解放前吧，于是基于最细维度分类的想法依然主导着我对MMStat的设计思考。下面就是我在MMStat中的做法：用最细（最长）的维度值组合（称为分类ID）作为key，把属于该key下的数据作为value，如下图所示：
![日志记录信息提取示意图](/assets/extract_log_record_info.png)  
  
如上图所示，对每一条日志记录MMStat都会提取他的维度值，然后把这些维度值组合成一个分类ID；同时还会提取要统计的数据，就目前业务来说图上的五个数据已经足够了，其实要统计的是什么数据，也被纳入到维度概念里面，图中statType维度的值是singleGoods（单个宝贝浏览页面），但具体的实现中其实这里面可能是singleGoodsCNT、singleGoodsDID、singleGoodsUDID、singleGoodsIP和singleGoodsUID，分别对应__单宝浏览PV__、**单宝浏览设备号**、**单宝浏览UDID**、**单宝浏览IP**和**单宝浏览的登陆用户ID**。至于产生统计项和匹配要处理的统计项这一结构在后面的文章中再细说，这里目的是阐述日志记录的维度值组合提取。  
  
当每一个分类ID对应的数据都被聚合起来后，MMStat会把这样的数据写入到HDFS作为元数据（也称作分类数据），这个数据就对应着过去python统计系统中的元数据文件，但是就没有索引文件了。
