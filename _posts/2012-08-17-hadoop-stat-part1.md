---
layout: default
published: true
---

# Hadoop统计平台——MMStat（一）  

## 抽象视图

不知不觉大半年都一直在负责统计，不过还包括统计自动化系统的设计，而不但只是跑跑脚本，虽然这种事情也得经常做。至于这个系统为什么叫MMStat，是我刚才突然想到的命名，反正就我一个人设计，一个人实现，给她取个名字也应该。  

长时间下来在这个统计系统的设计、开发和维护等方面都有不少感想，不过忙碌的工作让我也没时间把这些都整理下来，最近终于有这个机会了。这是一个线下的统计系统，依赖于前段服务器产生的日志信息进行各种流量统计。目前这个系统可以在一个小时内完成每日大量的统计计算，而目前每日的日志量大概是70G。  

下面是一幅简单的示意图，表示MMStat是如何获取线上日志数据的：  
![获取日志结构](/assets/get_log_flow.png)  

目前使用的下载方式是scp，*这样的架构需要维护掌握的前段服务器名字，因此这里过去曾经出现过几次因为前段机器已经上线了，但没有通知我，于是新增机器的日志就没有被下载，如果延误超过一天，那么就会出现数据统计错误*。所以这里应该形成机制，只要有新的服务器上线，就应该通知到维护统计系统的人。  

目前在HDFS中采用2备份的存储方式，连3备份都用不上，因为集群机器只有6台，也就是说只有5台是用来存储数据的。  

接下来就是统计任务执行过程的数据流图：
![计算任务数据流](/assets/compute_data_flow.png)  

每一个计算结果最终都存储到关系数据库（目前使用的mysql）中提供给外部应用程序使用，目前就是统计后台。至于中间结果目前是人工操作去删除。这个结构的设计是有历史原因的，在基于Hadoop的统计系统完成之前，我用python开发了另一个7000多行的单机统计系统（啊～说起这个统计系统，也犹如滔滔江水，延绵不绝，在此省略10万字），最大程度地利用16个CPU来进行统计计算，在那个系统里面就有一个设计错误的地方——直接把计算出来的数据写入到数据库。  

这种设计其实是基于马克思列宁主义的思想，那就是前端服务器的工程师和客户端APP的工程师绝对不会做错任何事情，那么我写好的统计程序代码就可以每天正确地产生计算结果，然后直接写入到数据库。地球人都知道这是不可能的事情，相反他们产生非常多的错误，例如日志输出错了、另一个APP却用了别的APP的agent、服务端和客户端增加了接口，而该接口是应当纳入统计的，但没有告诉我等等这些原因都会导致同一个结果——统计出错误的数据，我得修改统计程序，删除数据库记录，重新跑统计并观察中间结果。

做统计的人上辈子是折翼的天使，因为前端服务和客户端发现一个BUG，他们修改了就好了，而做统计的人需要在程序里面兼容所有BUG，因为leader不会怜悯地跟数据挖掘工程师说，“哎，看你这么苦逼，过去的数据就别统计了”，他们会当什么事情也没发生，世界一样美丽，让你把所有历史数据都跑出来。  

回到刚才的主题，正是因为在现实的环境下，总有各种各样的原因需要我去审核计算结果，根据这些日子的经验，80%的情况是先发现统计数据有问题，然后在计算结果里面检查，反向追踪程序运行流程，最后推导出前端服务获取客户端的程序出现了BUG。这里面印象比较深刻的一个例子就是有一个版本，客户端工程师在屏幕滑动的策略上做了调整，导致屏幕更难滑动，但当时全世界都觉得这是没什么问题的一个改动，当这个版本发布之后，相关的PV明显降低，然后大家都没有头绪。最后是通过用户session计算的中间结果，由leader亲自查看（其实这应该是我做的事情，但我一个负责全部统计，又不是再世哪咤，所以让leader自己干去！），发现滑动的日志数量大减，于是一切都明朗了。

所以在MMStat中，我把系统设计成可以简单分阶段产生中间结果的结构，从此世界美好了那么一点。这篇blog主要是对MMStat进行最简单的抽象介绍，后面的文章会更详细地讲解其他方面的内容。  